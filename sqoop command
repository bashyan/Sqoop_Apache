 LIST DATABASES
-------------
[hduser@ubuntu ~]$ sqoop list-databases --connect
jdbc:mysql://localhost --username root --password '';

LIST TABLES in a database
--------------------------
[hduser@ubuntu ~]$ sqoop list-tables --connect
jdbc:mysql://localhost/college --username root --password '';


Import one table from mysql into HDFS
-------------------------------------
[hduser@ubuntu ~]$ sqoop import --connect
jdbc:mysql://localhost/college --username root --password '' --table
student_master --target-dir /user/college/student_master;


sqoop import --connect jdbc:mysql://localhost/sales --username root
--password '' --table customer --target-dir /user/sales/customer -m 1;


WITH WHERE CLAUSE
--------------------------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --table student_master --where 'student_id=1'
--target-dir /user/college/query -m 1;


WITH QUERY
------------------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --query 'select * from student_master where $CONDITIONS
and student_id=2' --target-dir /user/college/query1 -m 1;


WITH JOINS in form of Query
-------------------------------------------
sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --query 'select a.student_id, a.name, a.address,
b.result from student_master a, fy b where $CONDITIONS and
a.student_id=b.student_id' --target-dir /user/college/query3 -m 1;






Import all tables from mysql into hdfs
----------------------------------------------------

[hduser@ubuntu ~]$ sqoop import-all-tables --connect
jdbc:mysql://localhost/college --username root --password '' -m 1;

(if target dir not specified the files will get copied in a folder by
the name hduser)

Import into hive managed tables
--------------------------------------
First create a database college in hive
hive> create database college;
hive> quit;

sqoop import --connect jdbc:mysql://localhost/college --username  root
--password '' --table student_master --hive-import --hive-table
college.student_profile -m 1;

sqoop import --connect jdbc:mysql://localhost/college --username  root
--password '' --table fy --hive-import --hive-table college.fyresults
-m 1;


Importing Data into Hbase using Sqoop
------------------------------------------------------
$ hbase shell
hbase > create 'college', 'student_profile', 'fyresults'
hbase>describe 'college'
hbase> exit

sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --table student_master --columns
"student_id,name,address" --hbase-table college --column-family
student_profile --hbase-row-key student_id  -m 1;

sqoop import --connect jdbc:mysql://localhost/college --username root
--password '' --table fy --columns "student_id,result" --hbase-table
college --column-family fyresults --hbase-row-key student_id  -m 1;

$ hbase shell
hbase > scan 'college'




sqoop export command
-----------------------------
nano employee.txt
1201,Rahul,Mumbai
1202,Sanjay,Bangalore
1203,Ravi,Pune
1204,Sanjana,Delhi
1205,Reena,Mumbai

hadoop fs -put employee.txt /user/hduser

In MySQl
------------------------------
create database employee;
use employee;
CREATE TABLE employee_master(
   employee_id INT NOT NULL AUTO_INCREMENT,
   name VARCHAR(40) NOT NULL,
   address VARCHAR(40) NOT NULL,
   PRIMARY KEY ( employee_id ));

sqoop export --connect jdbc:mysql://localhost/employee --username root
--password '' --table employee_master --export-dir /user/hduser/employee.txt
